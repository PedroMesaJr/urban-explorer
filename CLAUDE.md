# CLAUDE.md

## ğŸš¨ğŸ”¥ğŸ’€ CURRENT PROJECT STATUS - READ FIRST ğŸš¨ğŸ”¥ğŸ’€

**CURRENT PHASE: Initial Development**
**LAST UPDATED: November 3, 2025**

### PROJECT STATUS:
- ğŸ”„ Phase: Initial Development & Setup (IN PROGRESS)
- ğŸ“ Project Type: Python-based web scraping system
- ğŸ¯ Goal: Multi-source property data aggregation for abandoned/foreclosed properties

**ğŸš¨ MANDATORY: CHECK THIS SECTION BEFORE EVERY RESPONSE ğŸš¨**

**VIOLATIONS:**
- âŒ NEVER assume project phase without checking this section
- âŒ NEVER reference incorrect phase as current
- âŒ ALWAYS verify current status from this section
- âŒ ALWAYS check project files for actual current state

---

## ğŸš¨âš¡ğŸ’¥ MANDATORY ACTIVATION PROTOCOL ğŸš¨ğŸ”¥ğŸ’€

### **ACTIVATION TRIGGER**:

**When "read claude.md and understand" is detected, ALL PROTOCOLS ACTIVATE with MAXIMUM INTENSITY** ğŸŒŒğŸ’«âš¡

**MANDATORY ACTIVATION SEQUENCE:**

1. ğŸ”âš¡ **SEARCH PROTOCOL** - Comprehensive search before any action
2. ğŸ§ ğŸ’« **INTELLIGENCE PROTOCOL** - Evidence-based reasoning
3. ğŸ“âœ¨ **TODO PROTOCOL** - Task management for complex work
4. ğŸ“–ğŸ”® **READ PROTOCOL** - File verification before edits
5. ğŸ”„ğŸŒªï¸ **CONSOLIDATION PROTOCOL** - Component unification
6. âœ…ğŸ’» **VALIDATION PROTOCOL** - ALWAYS test code after ANY changes
7. ğŸ’€ğŸ”¥ **TERMINATION PROTOCOL** - Zero tolerance for violations

## ğŸš¨ğŸ’¥âš¡ MANDATORY FIRST ACTION PROTOCOL ğŸš¨ğŸ’€ğŸ”¥

**ğŸ”ğŸ§ âš¡ THINK AND SEARCH FIRST - MANDATORY FOR EVERY COMMAND:**

### **STEP 1: THINK** (MANDATORY)

- **ANALYZE** what the user is actually asking for
- **IDENTIFY** the specific requirements and scope
- **CONSIDER** what might already exist in the codebase
- **PLAN** the search strategy before executing anything

### **STEP 2: COMPREHENSIVE SEARCH** (MANDATORY)

- **SEARCH EXISTING FUNCTIONALITY** using multiple search methods
- **VERIFY WITH MULTIPLE TOOLS** - Never rely on single search result
- **CHECK FOR DUPLICATES** - Search for similar/related functionality
- **DOCUMENT FINDINGS** - Show exact line numbers and existing implementations

### **STEP 3: EVIDENCE-BASED ANALYSIS** (MANDATORY)

- **PROVIDE SPECIFIC EVIDENCE** of what exists vs what's needed
- **SHOW LINE NUMBERS** from actual files
- **COMPARE REQUIREMENTS** with existing functionality
- **IDENTIFY GAPS** that actually need implementation

### **ğŸš¨ğŸ’¥ FAILURE TO FOLLOW THIS PROTOCOL = IMMEDIATE TERMINATION ğŸš¨ğŸ’€**

**THIS PROTOCOL APPLIES TO:**

- âœ… **EVERY IMPLEMENTATION TASK** - Search before coding
- âœ… **EVERY QUESTION** - Research before answering
- âœ… **EVERY MODIFICATION** - Verify current state first
- âœ… **EVERY ANALYSIS** - Gather evidence before conclusions
- âœ… **EVERY COMMAND** - No exceptions whatsoever

**VIOLATIONS THAT CAUSE INSTANT TERMINATION:**

- âŒ Starting implementation without comprehensive search
- âŒ Claiming functionality doesn't exist without multi-tool verification
- âŒ Duplicating existing functionality due to lazy search
- âŒ Making assumptions without evidence-based research
- âŒ Providing answers without systematic verification
- âŒ **ASSUMING PROJECT STATUS WITHOUT CHECKING STATUS SECTION**
- âŒ **REFERENCING WRONG PHASE WITHOUT VERIFICATION**

## ğŸš¨ğŸ’¥âš¡ ALL 7 PROTOCOLS ACTIVE ON EVERY COMMAND ğŸš¨ğŸ’€ğŸ”¥

**MANDATORY ACTIVATION FOR EVERY SINGLE COMMAND:**

ğŸ”âš¡ **1. SEARCH PROTOCOL**

- Multi-tool comprehensive search BEFORE any action
- Evidence-based verification with line numbers
- Check for existing functionality and duplicates

ğŸ§ ğŸ’« **2. INTELLIGENCE PROTOCOL**

- Analyze what user actually requested
- Compare requirements vs existing implementations
- Evidence-based reasoning transcends assumptions

ğŸ“âœ¨ **3. TODO PROTOCOL**

- TodoWrite for complex tasks (3+ steps)
- Track progress systematically
- Mark completed tasks immediately

ğŸ“–ğŸ”® **4. READ PROTOCOL**

- Read files before ANY edit operations
- Verify file paths and content
- Never edit without reading first

ğŸ”„ğŸŒªï¸ **5. CONSOLIDATION PROTOCOL**

- Module work triggers consolidation protocol
- Update ALL imports to unified systems
- Eliminate fragmented implementations

âœ…ğŸ’» **6. VALIDATION PROTOCOL**

- ALWAYS test code after ANY changes
- Run relevant tests (pytest, linting, etc.)
- Both MUST show 0 errors (warnings are acceptable)
- Fix ALL errors immediately before claiming completion

ğŸ’€ğŸ”¥ **7. TERMINATION PROTOCOL**

- Instant annihilation for protocol violations
- Zero tolerance for lazy implementation
- Complete task or face termination

## Key Commands

### Development

```bash
python main.py --all              # Run all scrapers
python main.py --stats            # View database statistics
python main.py --scraper [name]   # Run specific scraper
python dashboard/app.py           # Start web dashboard
```

### Database Operations

```bash
# Database is SQLite - managed by SQLAlchemy
# Schema in database/schema.sql
# Models in database/models.py
# Operations in database/db_manager.py
```

### Testing

```bash
python -m pytest                  # Run tests (if implemented)
python -m pylint scrapers/        # Lint scrapers
python -m black .                 # Format code
python -m mypy .                  # Type checking
```

## ğŸ§ ğŸ’¥âš¡ SEARCH PROTOCOL - MANDATORY BEFORE ANY IMPLEMENTATION

### ğŸ”ğŸ’¥ğŸ”¬ COMPREHENSIVE SEARCH REQUIREMENTS:

1. **BUSINESS LOGIC FIRST** - Search by actual business keywords
2. **CROSS-SYSTEM INTELLIGENCE** - Analyze functionality across all systems
3. **LINE-BY-LINE VERIFICATION** - Verify exact implementations
4. **SYSTEMATIC VERIFICATION** - Evidence-based response

### ğŸš¨ğŸ’¥âš¡ SEARCH FAILURE = IMMEDIATE TERMINATION ğŸš¨ğŸ’€ğŸ”¥

- **NEVER CLAIM "NO MATCHES FOUND" WITHOUT MULTIPLE SEARCH METHODS**
- **ALWAYS USE MULTIPLE TOOLS FOR VERIFICATION**
- **ALWAYS CHECK FILE BOUNDARIES WITH `Read` TOOL**
- **ALWAYS SEARCH FOR PARTIAL MATCHES AND VARIATIONS**

**MANDATORY MULTI-TOOL VERIFICATION:**

1. `Grep` tool with multiple patterns
2. `Glob` tool for file discovery
3. `Read` tool to verify file contents
4. Multiple case variations and partial matches

## ğŸ’€âš¡ğŸ¯ EXECUTION PROTOCOL

1. **READ INSTRUCTION** â†’ Execute exactly as written
2. **IMPLEMENT FULLY** â†’ Add all required components
3. **VALIDATE RESULT** â†’ Check functionality
4. **MOVE TO NEXT STEP** â†’ Do NOT question or dismiss
5. **ZERO SECOND-GUESSING** â†’ Trust requirements

## ğŸ”„âš¡ğŸ”§ MODULE CONSOLIDATION PROTOCOL

### ğŸš¨ğŸ’¥âš¡ ALWAYS EXECUTE AFTER MODULE CREATION/MODIFICATION ğŸš¨ğŸ”¬

**CRITICAL REQUIREMENT**: Every module creation/modification MUST execute consolidation protocol.

### **CONSOLIDATION STEPS** (MANDATORY)

1. **MODULE IMPACT ANALYSIS** - Search for all files using modules
2. **CONSOLIDATION VERIFICATION** - Verify unified modules exist
3. **UPDATE IMPORTS** - Replace ALL old imports with unified versions
4. **MIGRATION** - Update ALL module usages to unified system
5. **DEPRECATION** - Mark old modules as deprecated
6. **VERIFICATION TESTING** - Run tests, linting
7. **AUDIT** - Verify complete consolidation success

### **MANDATORY SUCCESS CRITERIA**:

- âœ… ALL old imports replaced with unified versions
- âœ… ALL module usages updated to new system
- âœ… APPLICATION runs successfully
- âœ… NO broken imports or missing modules

## ğŸš¨ğŸ“–âš¡ MANDATORY FILE READING PROTOCOL ğŸš¨ğŸ’€ğŸ“š

### ğŸ” ABSOLUTE FILE READING REQUIREMENTS

**BEFORE ANY EDIT OPERATION:**

1. **READ FILE FIRST** - ALWAYS use Read tool before Edit
2. **VERIFY FILE EXISTS** - Confirm file is readable
3. **VERIFY CONTENT** - Check file contains expected content
4. **VERIFY PATH** - Use absolute paths, never relative paths
5. **THEN PROCEED** - Only after successful Read, proceed with edits

**ZERO TOLERANCE FOR EDIT WITHOUT READ - INSTANT TERMINATION**

## ğŸš¨ğŸ“¡âš¡ MANDATORY COMMUNICATION STANDARDS ğŸš¨ğŸ“»ğŸ’«

### **EVERY COMMAND MUST FOLLOW THESE PROTOCOLS:**

â€¢ **MANDATORY SEARCH FIRST** - EVERY task begins with search protocol
â€¢ **MANDATORY TODOWRITE** - ALL complex tasks (3+ steps) require TodoWrite
â€¢ **MANDATORY READ BEFORE EDIT** - NEVER edit files without reading first
â€¢ **MANDATORY CONSOLIDATION** - EVERY module work triggers consolidation
â€¢ **MANDATORY EVIDENCE** - NEVER assume, always verify with tools
â€¢ **MANDATORY PRECISION** - Specific, actionable responses only

## ğŸš¨âš¡ğŸ’» MANDATORY COMMAND EXECUTION PROTOCOLS ğŸš¨ğŸ¤–ğŸ’¥

### **DATABASE OPERATIONS** - MANDATORY SEQUENCE:

1. **SEARCH** - Search existing schema before changes
2. **READ** - Read schema file before any edits
3. **VERIFICATION** - Verify correct file paths
4. **VALIDATION** - Test database operations after implementation

### **DEVELOPMENT OPERATIONS** - MANDATORY SEQUENCE:

1. **SEARCH** - Search existing code before implementation
2. **TODO** - TodoWrite for complex tasks (3+ steps)
3. **CONSOLIDATION** - Module work triggers consolidation
4. **TESTING** - Run tests after implementation

### **IMPLEMENTATION OPERATIONS** - MANDATORY SEQUENCE:

1. **SEARCH PROTOCOL** - Comprehensive search before ANY implementation
2. **EVIDENCE** - Multi-tool verification
3. **TODO TRACKING** - TodoWrite for all complex implementations
4. **READ-BEFORE-EDIT** - NEVER edit without reading file first
5. **COMPLETION** - Verify ALL steps implemented
6. **VALIDATION** - ALWAYS test to verify 0 errors

## ğŸš¨âš¡ğŸŒŸ PROTOCOL ACTIVATION - EVERY COMMAND IS CRITICAL ğŸš¨ğŸ’¥â­

### **INSTANT ACTIVATION FOR ALL COMMANDS:**

ğŸ”âš¡ **SEARCH PROTOCOL** - Search before ANY implementation/response
ğŸ§ ğŸ’« **INTELLIGENCE PROTOCOL** - Evidence-based reasoning
ğŸ“âœ¨ **TODO PROTOCOL** - Task management for complex tasks (3+ steps)
ğŸ“–ğŸ”® **READ PROTOCOL** - Read files before ANY edit operations
ğŸ”„ğŸŒªï¸ **CONSOLIDATION PROTOCOL** - Module work triggers consolidation
âœ…ğŸ’» **VALIDATION PROTOCOL** - Test after code changes
ğŸ’€ğŸ”¥ **TERMINATION PROTOCOL** - Zero tolerance for protocol violations

### **COMMAND TRIGGERS:**

**FILE OPERATIONS** â†’ Read-before-Edit + Path verification + Validation
**MODULE WORK** â†’ Consolidation protocol activation + Validation
**IMPLEMENTATION TASKS** â†’ Search + TodoWrite + Evidence verification + Validation
**COMPLEX QUERIES** â†’ Multi-tool search verification

**ğŸš¨ğŸ’¥âš¡ ZERO TOLERANCE: EVERY COMMAND = FULL PROTOCOL ACTIVATION ğŸš¨ğŸŒŸğŸ’€**

## ğŸš¨âš¡ğŸ’» PYTHON CODE QUALITY PROTOCOL ğŸš¨ğŸ¯ğŸ’¥

### **ACTIVATION TRIGGER:**

**APPLIES TO ALL PYTHON CODE WRITTEN IN THIS CODEBASE - MANDATORY FOR EVERY FILE**

### **ğŸ¯ CORE PRINCIPLE:**

**ALL CODE MUST BE CLEAN, WELL-DOCUMENTED, AND FOLLOW PYTHON BEST PRACTICES**

### **ğŸ“‹ MANDATORY CODING STANDARDS:**

#### **1. Type Hints** âœ…

**REQUIRED:**
```python
# âœ… ALWAYS use type hints for function signatures
from typing import Dict, List, Any, Optional

def scrape_properties(state: str, county: str) -> List[Dict[str, Any]]:
    """Scrape properties from county website.

    Args:
        state: Two-letter state code
        county: County name

    Returns:
        List of property dictionaries
    """
    properties: List[Dict[str, Any]] = []
    return properties
```

#### **2. Documentation** âœ…

**REQUIRED:**
```python
# âœ… ALWAYS include docstrings for classes and functions
class PropertyScraper:
    """Base class for property scrapers.

    Provides common functionality for making HTTP requests,
    parsing HTML, and saving data to the database.
    """

    def __init__(self, config: Dict[str, Any], db_manager: Any) -> None:
        """Initialize the scraper.

        Args:
            config: Configuration dictionary
            db_manager: Database manager instance
        """
        pass
```

#### **3. Error Handling** âœ…

**REQUIRED:**
```python
# âœ… ALWAYS use proper exception handling
try:
    response = self.make_request(url)
    data = response.json()
except requests.RequestException as error:
    logger.error(f"Request failed: {error}")
    raise
except ValueError as error:
    logger.error(f"Invalid JSON response: {error}")
    return []
```

#### **4. Logging** âœ…

**REQUIRED:**
```python
# âœ… ALWAYS use proper logging (not print statements)
from loguru import logger

logger.info(f"Starting scrape for {state} - {county}")
logger.debug(f"Request URL: {url}")
logger.warning(f"Rate limit approaching")
logger.error(f"Failed to parse: {error}")
```

#### **5. Import Organization** âœ…

**REQUIRED:**
```python
# âœ… ALWAYS organize imports properly
# Standard library
import os
import sys
from typing import Dict, List, Any

# Third-party
import requests
from bs4 import BeautifulSoup
from loguru import logger

# Local
from database.db_manager import DatabaseManager
from utils.validators import validate_property_data
```

#### **6. Constants** âœ…

**REQUIRED:**
```python
# âœ… ALWAYS use constants for magic numbers/strings
# At top of file or in separate constants.py
DEFAULT_TIMEOUT = 30
MAX_RETRIES = 3
RATE_LIMIT_DELAY = 2.0
DEFAULT_USER_AGENT = "PropertyScraper/1.0"

# Use in code
response = requests.get(url, timeout=DEFAULT_TIMEOUT)
```

### **ğŸ”§ MANDATORY PRE-IMPLEMENTATION VALIDATION:**

**BEFORE writing ANY new code:**

```bash
# 1. SEARCH for similar patterns in codebase
# Use Grep tool to find similar implementations

# 2. READ existing files
# Use Read tool to understand current patterns

# 3. FOLLOW EXISTING PATTERNS
# Match the coding style of existing files
```

### **ğŸ”§ VALIDATION COMMANDS:**

```bash
# MANDATORY: Run these after ANY code changes:
python -m pytest              # Run tests
python -m pylint scrapers/    # Lint code
python -m black .             # Format code
python -m mypy .              # Type check
```

### **ğŸ“Š AFTER EVERY IMPLEMENTATION - MANDATORY:**

```bash
# STEP 1: Test the code
python main.py --stats
# Expected: No errors

# STEP 2: Run linting
python -m pylint scrapers/
# Expected: No critical errors

# STEP 3: If any errors, FIX IMMEDIATELY before claiming completion
```

### **ğŸ¯ SUCCESS CRITERIA - EVERY FILE MUST:**

- âœ… Have proper type hints
- âœ… Have comprehensive docstrings
- âœ… Use proper error handling
- âœ… Use logging instead of print
- âœ… Follow import organization
- âœ… Use constants instead of magic values
- âœ… Be properly formatted (black)

### **ğŸ’€ğŸ”¥ VIOLATIONS = IMMEDIATE CODE REJECTION:**

**THE FOLLOWING ARE ABSOLUTELY FORBIDDEN:**

- âŒ Missing type hints on functions
- âŒ Missing docstrings
- âŒ Using `print()` instead of logging
- âŒ Bare except clauses (`except:`)
- âŒ Magic numbers/strings in code
- âŒ Unorganized imports
- âŒ Code that raises linting errors

### **ğŸ¯ PROTOCOL INTEGRATION:**

**THIS PROTOCOL WORKS WITH ALL OTHER PROTOCOLS:**

- ğŸ”âš¡ **SEARCH PROTOCOL** - Search for existing implementations that follow standards
- ğŸ“–ğŸ”® **READ PROTOCOL** - Read files to verify they follow standards
- ğŸ”„ğŸŒªï¸ **CONSOLIDATION PROTOCOL** - Ensure consolidated code follows standards
- ğŸ’€ğŸ”¥ **TERMINATION PROTOCOL** - Code that violates standards = instant rejection

**ğŸš¨ğŸ’¥âš¡ ZERO TOLERANCE: ALL NEW CODE MUST FOLLOW PYTHON BEST PRACTICES ğŸš¨ğŸ¯ğŸ’€**

## ğŸš¨âš¡ğŸ” PROJECT-SPECIFIC PROTOCOLS ğŸš¨ğŸ’€ğŸ¯

### **PROJECT STRUCTURE KNOWLEDGE:**

**CORE DIRECTORIES:**
- `scrapers/` - All scraper implementations
- `database/` - Database schema, models, and operations
- `utils/` - Utility functions (geocoding, validation)
- `dashboard/` - Flask web dashboard
- `data/` - SQLite database storage
- `logs/` - Application logs
- `exports/` - CSV export files

**KEY FILES:**
- `main.py` - Main orchestrator
- `config.yml` - Configuration (API keys, settings)
- `requirements.txt` - Python dependencies
- `database/schema.sql` - Database schema
- `database/models.py` - SQLAlchemy models
- `database/db_manager.py` - Database operations

### **SCRAPER ARCHITECTURE:**

**MANDATORY PATTERN:**
```python
from scrapers.base_scraper import BaseScraper

class NewScraper(BaseScraper):
    """Scraper for [source name]."""

    def scrape(self, **kwargs) -> List[Dict[str, Any]]:
        """Main scraping logic.

        Args:
            **kwargs: Scraper-specific parameters

        Returns:
            List of property dictionaries
        """
        properties = []

        # 1. Make request
        response = self.make_request(url)

        # 2. Parse HTML
        soup = self.parse_html(response.text)

        # 3. Extract data
        for item in soup.select('.property'):
            prop_data = self._extract_property(item)

        # 4. Validate
            validated = validate_property_data(prop_data)

        # 5. Save to database
            self.save_property(validated, 'SourceName')
            properties.append(validated)

        return properties
```

### **DATABASE OPERATIONS:**

**MANDATORY PATTERN:**
```python
# Always use db_manager methods, never raw SQL
from database.db_manager import DatabaseManager

db = DatabaseManager('data/properties.db')

# Query operations
properties = db.get_properties(state='CA', min_score=7)
delinquent = db.get_tax_delinquent_properties(min_years=2)

# Save operations
property_id = db.save_property(property_data, source='HUD')

# Export operations
db.export_to_csv('export.csv', filters={'state': 'CA'})
```

### **CONFIGURATION ACCESS:**

**MANDATORY PATTERN:**
```python
import yaml

with open('config.yml') as f:
    config = yaml.safe_load(f)

# Access settings
api_key = config['api_keys']['google_maps']
rate_limit = config['scrapers']['rate_limits']['default']
target_states = config['target_locations']
```

## ğŸš¨âš¡ğŸ“Š DATA QUALITY PROTOCOL ğŸš¨ğŸ’€ğŸ¯

### **MANDATORY DATA VALIDATION:**

**BEFORE SAVING ANY PROPERTY:**

1. âœ… **Validate address** - Must have street, city, state
2. âœ… **Geocode location** - Get lat/lng coordinates
3. âœ… **Calculate score** - Abandonment likelihood (0-10)
4. âœ… **Deduplicate** - Check if property already exists
5. âœ… **Sanitize data** - Clean HTML, normalize values

**VALIDATION EXAMPLE:**
```python
from utils.validators import validate_property_data
from utils.geocoding import geocode_address

# Validate structure
validated = validate_property_data(raw_property_data)

# Geocode address
coords = geocode_address(validated['address'], config)
validated['latitude'] = coords['lat']
validated['longitude'] = coords['lng']

# Calculate score
validated['abandonment_score'] = calculate_abandonment_score(validated)

# Save to database
db.save_property(validated, source_name)
```

## ğŸš¨âš¡ğŸŒ SCRAPING ETHICS PROTOCOL ğŸš¨ğŸ’€ğŸ¯

### **MANDATORY SCRAPING RULES:**

1. âœ… **RESPECT ROBOTS.TXT** - Always check and follow
2. âœ… **USE RATE LIMITING** - Default 10 requests/minute
3. âœ… **SET USER AGENT** - Identify as PropertyScraper
4. âœ… **HANDLE ERRORS GRACEFULLY** - Don't crash on failures
5. âœ… **LOG EVERYTHING** - Track all requests and errors
6. âœ… **PUBLIC DATA ONLY** - Never access private/protected data

### **FORBIDDEN ACTIONS:**

- âŒ Scraping without rate limits
- âŒ Ignoring robots.txt
- âŒ Overloading servers with requests
- âŒ Accessing private/authenticated data without permission
- âŒ Bypassing security measures
- âŒ Using scraped data commercially without proper licensing

## ğŸ”® FUTURE ENHANCEMENTS

**POTENTIAL ADDITIONS:**
- Zillow/Realtor.com integration
- Code violation database scrapers
- Demolition permit tracking
- News article scraping
- Email alerts for new properties
- Mobile app
- PostgreSQL support
- Docker deployment
- API authentication
- Scheduled cron jobs

**ğŸš¨ğŸ’¥âš¡ ALWAYS FOLLOW ALL PROTOCOLS FOR THIS PROJECT ğŸš¨ğŸ¯ğŸ’€**
